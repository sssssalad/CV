{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"398424dfac1d4bc9b06a5b8fb89d6f08":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_73c5810ecfff433fbc3d6828f00e3882","IPY_MODEL_0879a877108c4b119bb28c2f4407f0d7","IPY_MODEL_c1ab2f988a9b4ae09f61ef1def1ba243"],"layout":"IPY_MODEL_be3cbccbdba44d678bcc4aaa72f933aa"}},"73c5810ecfff433fbc3d6828f00e3882":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b147cf2d05de49fa89ac925aa50ce6ea","placeholder":"​","style":"IPY_MODEL_6d0a74ae147a4e4799cf0a2c9624b2e3","value":"100%"}},"0879a877108c4b119bb28c2f4407f0d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6675cafb9e14bb6a5b645fed1887902","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1354beeb2ab24e90bda17ad29041d620","value":170498071}},"c1ab2f988a9b4ae09f61ef1def1ba243":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d1bd8eb45724e42bbbcb621adacbc1c","placeholder":"​","style":"IPY_MODEL_9767b3d505354bdba8df980586f6a827","value":" 170498071/170498071 [00:03&lt;00:00, 53436256.83it/s]"}},"be3cbccbdba44d678bcc4aaa72f933aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b147cf2d05de49fa89ac925aa50ce6ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d0a74ae147a4e4799cf0a2c9624b2e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6675cafb9e14bb6a5b645fed1887902":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1354beeb2ab24e90bda17ad29041d620":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d1bd8eb45724e42bbbcb621adacbc1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9767b3d505354bdba8df980586f6a827":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"18bUwTQ8zuhb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671891629558,"user_tz":-480,"elapsed":8581,"user":{"displayName":"salad xx","userId":"13786222985564916523"}},"outputId":"cfb73386-013a-4b72-f599-35143519067f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting einops\n","  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n","\u001b[K     |████████████████████████████████| 41 kB 496 kB/s \n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.6.0\n"]}],"source":["! pip install einops"]},{"cell_type":"code","source":["import torch\n","from torch import nn, einsum\n","import torch.nn.functional as F\n","\n","from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange\n","import numpy as np"],"metadata":{"id":"OPS4xoUB24XX","executionInfo":{"status":"ok","timestamp":1671891630186,"user_tz":-480,"elapsed":662,"user":{"displayName":"salad xx","userId":"13786222985564916523"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)"],"metadata":{"id":"IDay6rdW28Ey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fn = fn\n","    def forward(self, x, **kwargs):\n","        return self.fn(self.norm(x), **kwargs)"],"metadata":{"id":"jR1Bp9q92-gF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout=0.):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, dim), \n","            nn.Dropout(dropout)\n","        )\n","    def forward(self, x):\n","        return self.net(x)"],"metadata":{"id":"Wr9DUznX3ChK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Attention(nn.Module):              \n","    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n","        super().__init__()\n","        inner_dim = dim_head * heads\n","        project_out = not (heads == 1 and dim_head == dim)\n","\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","\n","        self.attend = nn.Softmax(dim=-1)\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n","\n","        self.to_out = nn.Sequential(\n","            nn.Linear(inner_dim, dim),\n","            nn.Dropout(dropout),\n","        ) if project_out else nn.Identity()\n","\n","    def forward(self, x):\n","        b, n, _, h = *x.shape, self.heads\n","        qkv = self.to_qkv(x).chunk(3, dim=-1)           # (b, n(65), dim*3) ---> 3 * (b, n, dim)\n","        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)          # q, k, v   (b, h, n, dim_head(64))\n","\n","        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n","\n","        attn = self.attend(dots)\n","\n","        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n","        out = rearrange(out, 'b h n d -> b n (h d)')\n","        return self.to_out(out)"],"metadata":{"id":"muSMqfKH3FUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n","        super().__init__()\n","        self.layers = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n","                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n","            ]))\n","    \n","    def forward(self, x):\n","        for attn, ff in self.layers:\n","            x = attn(x) + x\n","            x = ff(x) + x\n","        return x"],"metadata":{"id":"GfWd-itd3IxW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ViT(nn.Module):\n","    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool='cls', channels=3, dim_head=64, dropout=0., emb_dropout=0.):\n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","        patch_height, patch_width = pair(patch_size)\n","\n","        assert  image_height % patch_height ==0 and image_width % patch_width == 0\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        patch_dim = channels * patch_height * patch_width\n","        assert pool in {'cls', 'mean'}\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),\n","            nn.Linear(patch_dim, dim)\n","        )\n","\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches+1, dim))\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\t\t\t\t\t# nn.Parameter()定义可学习参数\n","        self.dropout = nn.Dropout(emb_dropout)\n","\n","        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n","\n","        self.pool = pool\n","        self.to_latent = nn.Identity()\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        x = self.to_patch_embedding(img)        # b c (h p1) (w p2) -> b (h w) (p1 p2 c) -> b (h w) dim\n","        b, n, _ = x.shape           # b表示batchSize, n表示每个块的空间分辨率, _表示一个块内有多少个值\n","\n","        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)  # self.cls_token: (1, 1, dim) -> cls_tokens: (batchSize, 1, dim)  \n","        x = torch.cat((cls_tokens, x), dim=1)               # 将cls_token拼接到patch token中去       (b, 65, dim)\n","        x += self.pos_embedding[:, :(n+1)]                  # 加位置嵌入（直接加）      (b, 65, dim)\n","        x = self.dropout(x)\n","\n","        x = self.transformer(x)                                                 # (b, 65, dim)\n","\n","        x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]                   # (b, dim)\n","\n","        x = self.to_latent(x)                                                   # Identity (b, dim)\n","        #print(x.shape)\n","\n","        return self.mlp_head(x) "],"metadata":{"id":"NsnE_GSU3Lkh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","model_vit = ViT(\n","        image_size = 224,\n","        patch_size = 16,\n","        num_classes = 10,\n","        dim = 256,\n","        depth = 2,\n","        heads = 4,\n","        mlp_dim = 128,\n","        dropout = 0.1,\n","        emb_dropout = 0.1\n",")\n","\n","img = torch.randn(16, 3, 256, 256)\n","\n","preds = model_vit(img) \n","print(preds.shape)\n","\"\"\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"id":"-3E_m6UE5vqy","executionInfo":{"status":"ok","timestamp":1671860182043,"user_tz":-480,"elapsed":12,"user":{"displayName":"salad xx","userId":"13786222985564916523"}},"outputId":"bb965600-adeb-4b64-a2d6-9b14b624759a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nmodel_vit = ViT(\\n        image_size = 224,\\n        patch_size = 16,\\n        num_classes = 10,\\n        dim = 256,\\n        depth = 2,\\n        heads = 4,\\n        mlp_dim = 128,\\n        dropout = 0.1,\\n        emb_dropout = 0.1\\n)\\n\\nimg = torch.randn(16, 3, 256, 256)\\n\\npreds = model_vit(img) \\nprint(preds.shape)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torchvision\n","import torchvision.transforms as transforms\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import numpy as np\n","#from vit_pytorch import ViT, SimpleViT\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def main():\n","    # 加载和预处理数据集\n","    trans_train = transforms.Compose(\n","        [transforms.RandomResizedCrop(224),  # 将给定图像随机裁剪为不同的大小和宽高比，然后缩放所裁剪得到的图像为制定的大小；\n","         # （即先随机采集，然后对裁剪得到的图像缩放为同一大小） 默认scale=(0.08, 1.0)\n","         transforms.RandomHorizontalFlip(),  # 以给定的概率随机水平旋转给定的PIL的图像，默认为0.5；\n","         transforms.ToTensor(),\n","         transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","                              std=[0.5, 0.5, 0.5])])\n","\n","    trans_valid = transforms.Compose(\n","        [transforms.Resize(256),  # 是按照比例把图像最小的一个边长放缩到256，另一边按照相同比例放缩。\n","         transforms.CenterCrop(224),  # 依据给定的size从中心裁剪\n","         transforms.ToTensor(),  # 将PIL Image或者 ndarray 转换为tensor，并且归一化至[0-1]\n","         # 归一化至[0-1]是直接除以255，若自己的ndarray数据尺度有变化，则需要自行修改。\n","         transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","                              std=[0.5, 0.5, 0.5])])  # 对数据按通道进行标准化，即先减均值，再除以标准差，注意是 hwc\n","\n","    trainset = torchvision.datasets.CIFAR10(root=\"./cifar10\", train=True, download=True, transform=trans_train)\n","    #trainset = torchvision.datasets.LFWPeople(root=\"./lfw\", split='train', download=True, transform=trans_train)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True)\n","\n","    testset = torchvision.datasets.CIFAR10(root='./cifar10', train=False, download=False, transform=trans_valid)\n","    #testset = torchvision.datasets.LFWPeople(root='./lfw', split='test', download=True, transform=trans_valid)\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=256,\n","                                             shuffle=False)\n","\n","   # classes = ('plane', 'car', 'bird', 'cat',\n","   #            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","    # 使用预训练模型\n","    model = ViT(\n","               image_size = 224,\n","               patch_size = 16,\n","               num_classes = 10,\n","               dim = 256,\n","               depth = 2,\n","               heads = 4,\n","               mlp_dim = 128,\n","               dropout = 0.1,\n","               emb_dropout = 0.1\n","    )\n","\n","    model = model.to(device)\n","    criterion = nn.CrossEntropyLoss()  # 损失函数\n","    # 只需要优化最后一层参数\n","    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-3, momentum=0.9)  # 优化器\n","\n","    # train\n","    train(model, trainloader, testloader, 5, optimizer, criterion)\n","\n","\n","# 计算准确率\n","def get_acc(output, label):\n","    total = output.shape[0]\n","    _, pred_label = output.max(1)\n","    num_correct = (pred_label == label).sum().item()\n","    return num_correct / total\n","\n","\n","# 显示图片\n","def imshow(img):\n","    img = img / 2 + 0.5  # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","\n","# 定义训练函数\n","def train(net, train_data, valid_data, num_epochs, optimizer, criterion):\n","    prev_time = datetime.now()\n","    for epoch in range(num_epochs):\n","        train_loss = 0\n","        train_acc = 0\n","        net = net.train()\n","        for im, label in train_data:\n","            im = im.to(device)  # (bs, 3, h, w)\n","            label = label.to(device)  # (bs, h, w)\n","            # forward\n","            output = net(im)\n","            loss = criterion(output, label)\n","            # backward\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            train_acc += get_acc(output, label)\n","\n","        cur_time = datetime.now()\n","        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n","        m, s = divmod(remainder, 60)\n","        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n","        if valid_data is not None:\n","            valid_loss = 0\n","            valid_acc = 0\n","            net = net.eval()\n","            for im, label in valid_data:\n","                im = im.to(device)  # (bs, 3, h, w)\n","                label = label.to(device)  # (bs, h, w)\n","                output = net(im)\n","                loss = criterion(output, label)\n","                valid_loss += loss.item()\n","                valid_acc += get_acc(output, label)\n","            epoch_str = (\n","                    \"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \"\n","                    % (epoch, train_loss / len(train_data),\n","                       train_acc / len(train_data), valid_loss / len(valid_data),\n","                       valid_acc / len(valid_data)))\n","        else:\n","            epoch_str = (\"Epoch %d. Train Loss: %f, Train Acc: %f, \" %\n","                         (epoch, train_loss / len(train_data),\n","                          train_acc / len(train_data)))\n","        prev_time = cur_time\n","        print(epoch_str + time_str)\n","\n","\"\"\"\n","def get_vit_model():\n","    v = SimpleViT(\n","        image_size=224,\n","        patch_size=16,\n","        num_classes=10,\n","        dim=256,\n","        depth=2,\n","        heads=4,\n","        mlp_dim=128\n","    )\n","    return v\n","\"\"\"\n","\n","if __name__ == '__main__':\n","    main()\n","\n"],"metadata":{"id":"j56W_RppJWIm","colab":{"base_uri":"https://localhost:8080/","height":193,"referenced_widgets":["398424dfac1d4bc9b06a5b8fb89d6f08","73c5810ecfff433fbc3d6828f00e3882","0879a877108c4b119bb28c2f4407f0d7","c1ab2f988a9b4ae09f61ef1def1ba243","be3cbccbdba44d678bcc4aaa72f933aa","b147cf2d05de49fa89ac925aa50ce6ea","6d0a74ae147a4e4799cf0a2c9624b2e3","b6675cafb9e14bb6a5b645fed1887902","1354beeb2ab24e90bda17ad29041d620","4d1bd8eb45724e42bbbcb621adacbc1c","9767b3d505354bdba8df980586f6a827"]},"executionInfo":{"status":"ok","timestamp":1671860932686,"user_tz":-480,"elapsed":750653,"user":{"displayName":"salad xx","userId":"13786222985564916523"}},"outputId":"68f0de16-8661-4467-f4b5-624b7cdd1c33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar10/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"398424dfac1d4bc9b06a5b8fb89d6f08"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./cifar10/cifar-10-python.tar.gz to ./cifar10\n","Epoch 0. Train Loss: 2.295467, Train Acc: 0.126933, Valid Loss: 2.228316, Valid Acc: 0.194629, Time 00:02:10\n","Epoch 1. Train Loss: 2.225699, Train Acc: 0.159395, Valid Loss: 2.155908, Valid Acc: 0.194336, Time 00:02:27\n","Epoch 2. Train Loss: 2.172329, Train Acc: 0.180987, Valid Loss: 2.098505, Valid Acc: 0.217676, Time 00:02:27\n","Epoch 3. Train Loss: 2.133742, Train Acc: 0.190354, Valid Loss: 2.077731, Valid Acc: 0.221680, Time 00:02:23\n","Epoch 4. Train Loss: 2.119933, Train Acc: 0.197417, Valid Loss: 2.062865, Valid Acc: 0.236230, Time 00:02:24\n"]}]}]}