{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJrhwUkazG8BkrwQmLhAzO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":12,"metadata":{"id":"TsHuom0Pjt8x","executionInfo":{"status":"ok","timestamp":1671894048869,"user_tz":-480,"elapsed":391,"user":{"displayName":"salad xx","userId":"13786222985564916523"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import pickle\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"markdown","source":["## 加载CIFAR-10数据集"],"metadata":{"id":"3r-XRIqklbSJ"}},{"cell_type":"code","source":["train_data = {b'data':[], b'labels':[]} #两个items都是list形式\n","# 5*10000的训练数据和1*10000的测试数据，数据为dict形式，train_data[b'data']为10000 * 3072的numpy向量\n","# 3072个数字表示图片特征，前1024个表示红色通道，中间1024表示绿色通道，最后1024表示蓝色通道\n","# train[b'labels']为长度为10000的list，每一个list数字对应以上上3072维的一个特征\n","\n","(X_train_full,y_train_full),(X_test,y_test) = keras.datasets.cifar10.load_data()\n","#10个类别，60000个32×32像素图像，50000个用于训练，10000个用于测试\n","\n","X_train = X_train_full[5000:]\n","y_train = y_train_full[5000:]\n","X_valid = X_train_full[:5000]\n","y_valid = y_train_full[:5000]\n","\n","#归一化输入特征\n","X_train = X_train/255.0\n","X_test = X_test/255.0\n","X_valid = X_valid/255.0\n","X_test = X_test/255.0\n","\n","# 加载训练数据\n","for i in range(5):\n","    with open(\"C:/Users/29811/Desktop/cifar10/data/cifar-10-batches-py/data_batch_\" + str(i + 1), mode='rb') as file:\n","        data = pickle.load(file, encoding='bytes')\n","        train_data[b'data'] += list(data[b'data'])\n","        train_data[b'labels'] += data[b'labels']\n","\n","# 加载测试数据\n","with open(\"C:/Users/29811/Desktop/cifar10/data/cifar-10-batches-py/test_batch\", mode='rb') as file:\n","    test_data = pickle.load(file, encoding='bytes')\n","\n","# 定义一些变量\n","NUM_LABLES = 10 # 分类结果为10类\n","FC_SIZE = 384   # 全连接隐藏层节点个数\n","BATCH_SIZE = 32 # 每次训练batch数\n","lamda = 0.004   # 正则化系数，这里还未用正则化处理\n","sess = tf.InteractiveSession()\n"],"metadata":{"id":"U5x9AnCYpmLQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 构建预处理函数"],"metadata":{"id":"xyU28r0Xk59I"}},{"cell_type":"code","source":["def _get_file_path(filename=''):\n","    \"\"\" \n","    Return the full path of a data-file for the data-set.\n","    If filename==\"\" then return the directory of the files.\n","    \"\"\"\n","    return os.path.join(data_path,'cifar-10-batches-py/',filename)"],"metadata":{"id":"zZNVSxzjkAqO","executionInfo":{"status":"ok","timestamp":1671893542851,"user_tz":-480,"elapsed":517,"user":{"displayName":"salad xx","userId":"13786222985564916523"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def _unpickle(filename):\n","    \"\"\" \n","    Unpickle the given file and return the data.\n","    Note that the appropriate dir-name is prepended\n","    the filename.\n","    \"\"\"\n","    file_path = _get_file_path(filename)\n","    print(\"Loading data: \" + file_path)\n","    with open(file_path, mode='rb') as file:\n","        if python_version == \"2\":\n","            data = pickle.load(file)\n","        else:\n","            data = pickle.load(file, encoding=\"bytes\")\n","    return data\n","\n","def _convert_images(raw):\n","    \"\"\"\n","    Convert images from unpickled data (10000, 3072)\n","    to a 4-dim array\n","    \n","    Args:\n","        raw: unpackled data from cifar10, eg: (10000,3072)\n","    return:\n","        a 4-dim array: (img_num, height, width, channel)\n","    \"\"\"\n","    num_channels = 3\n","    img_size = 32\n","    raw_float = np.array(raw, dtype=float)/255.0\n","    images = raw_float.reshape([-1,num_channels,img_size,img_size])\n","    images = images.transpose([0, 2, 3, 1])\n","    return images"],"metadata":{"id":"MlDMMoolkAsb","executionInfo":{"status":"ok","timestamp":1671893543236,"user_tz":-480,"elapsed":3,"user":{"displayName":"salad xx","userId":"13786222985564916523"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def _load_data(filename):\n","    \"\"\"\n","    Load a pickled data-file from the CIFAR-10 data set\n","    and return the converted images (see above) and the \n","    class-number for each image.\n","    \"\"\"\n","    data = _unpickle(filename)\n","    if python_version == \"2\":\n","        raw_images = data['data'] # delete 'b' when using python2\n","        labels = np.array(data['labels']) # delete 'b' when using python2\n","    else:\n","        raw_images = data[b'data']\n","        labels = np.array(data[b'labels'])  \n","    images = _convert_images(raw_images)\n","    return images, labels\n","\n","def load_label_names():\n","    \"\"\"\n","    Load the names for the classes in the CIFAR-10 data set.\n","    Returns a list with the names. \n","    Example: names[3] is the name associated with class-number 3.\n","    \"\"\"\n","    raw = _unpickle(\"batches.meta\")\n","    if python_version == \"2\":\n","        label_names = [x.decode('utf-8') for x in raw['label_names']]\n","    else:\n","        label_names = raw[b'label_names']\n","    return label_names\n","\n","def _one_hot_encoded(class_numbers, num_classes=None):\n","    \"\"\"\n","    Generate the One-Hot encoded class-labels from an array of integers.\n","\n","    For example, if class_number=2 and num_classes=4 then\n","    the one-hot encoded label is the float array: [0. 0. 1. 0.]\n","\n","    Args:\n","        class_numbers: array of integers with class-numbers.\n","        num_classes: number of classes. If None then use max(cls)-1.\n","    Return:\n","        2-dim array of shape: [len(cls), num_classes]\n","    \"\"\"\n","    if num_classes is None:\n","        num_classes = np.max(class_numbers)+1\n","        \n","    return np.eye(num_classes, dtype=float)[class_numbers] "],"metadata":{"id":"xaWhd4M0kAu0","executionInfo":{"status":"ok","timestamp":1671893543612,"user_tz":-480,"elapsed":2,"user":{"displayName":"salad xx","userId":"13786222985564916523"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def load_training_data():\n","    \"\"\"\n","    Load all the training-data for the CIFAR-10 data-set.\n","    The data-set is split into 5 data-files which are merged here.\n","\n","    Returns:\n","        images: training images\n","        labels: label of training images\n","        one_hot_labels: one-hot labels.\n","    \"\"\" \n","    num_files_train = 5\n","    images_per_file = 10000\n","    num_classes = 10\n","    img_size = 32\n","    num_channels = 3\n","    num_images_train = num_files_train*images_per_file\n","    \n","    # 32bit的Python使用内存超过2G之后,此处会报MemoryError(最好用64位)\n","    images = np.zeros(shape=[num_images_train, img_size, img_size, num_channels], dtype=float)\n","    labels = np.zeros(shape=[num_images_train], dtype=int)\n","    \n","    begin = 0\n","    for i in range(num_files_train):\n","        images_batch, labels_batch = _load_data(filename=\"data_batch_\"+str(i+1)) # _load_data2 in python2\n","        num_images = len(images_batch)\n","        end = begin + num_images\n","        images[begin:end,:] = images_batch\n","        labels[begin:end] = labels_batch\n","        begin = end\n","    one_hot_labels = _one_hot_encoded(class_numbers=labels,num_classes=num_classes)\n","    return images, labels, one_hot_labels"],"metadata":{"id":"mHPQvXI6kAxJ","executionInfo":{"status":"ok","timestamp":1671893544103,"user_tz":-480,"elapsed":3,"user":{"displayName":"salad xx","userId":"13786222985564916523"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def load_test_data():\n","    \"\"\"\n","    Load all the test-data for the CIFAR-10 data-set.\n","    Returns:\n","    the images, class-numbers and one-hot encoded class-labels.\n","    \"\"\"\n","    num_classes = 10\n","    images, labels = _load_data(filename=\"test_batch\") # _load_data2 in python2\n","    return images, labels, _one_hot_encoded(class_numbers=labels, num_classes=num_classes)"],"metadata":{"id":"HaDEM9kWkAzu","executionInfo":{"status":"ok","timestamp":1671893544103,"user_tz":-480,"elapsed":2,"user":{"displayName":"salad xx","userId":"13786222985564916523"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def split_train_data(images_train, one_hot_labels_train, ratio = 0.1, shuffle = False):\n","    \"\"\"\n","    split valid data from train data with specified ratio.\n","    \n","    Arguments:\n","        images_train: train data (50000, 32, 32, 3).\n","        one_hot_labels_train: one-hot labels of train data (50000, 10). \n","        ratio: valid data ratio.\n","        shuffle: shuffle or not.  \n","    Return:\n","        images_train: splitted train data.\n","        one_hot_labels_train: train data labels.\n","        images_valid: valid data\n","        one_hot_labels_valid: valid data labels\n","    \"\"\"\n","    num_train = images_train.shape[0]\n","    num_valid = int(np.math.floor(num_train * ratio))\n","\n","    if shuffle:\n","        permutation = list(np.random.permutation(num_train))\n","        images_train = images_train[permutation, ]\n","        one_hot_labels_train = one_hot_labels_train[permutation, ]\n","\n","    images_valid = images_train[-num_valid:, ]\n","    one_hot_labels_valid = one_hot_labels_train[-num_valid:, ]\n","    images_train = images_train[0:-num_valid, ]\n","    one_hot_labels_train = one_hot_labels_train[0:-num_valid, ]\n","    return images_train, one_hot_labels_train, images_valid, one_hot_labels_valid"],"metadata":{"id":"ImOdsD-jkA1_","executionInfo":{"status":"ok","timestamp":1671893544491,"user_tz":-480,"elapsed":3,"user":{"displayName":"salad xx","userId":"13786222985564916523"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def create_mini_batches(X, Y, mini_batch_size = 128, shuffle=False):\n","    \"\"\"\n","    Create a list of minibatches from the training images.\n","    \n","    Arguments:\n","        X: numpy.ndarry images shaped (num_images, height, width, channels).\n","           for example: (50000, 32, 32, 3).\n","        Y: one-hot labels of images shaped (num_images, num_classes).\n","           for example: (50000,10) \n","        mini_batch_size: Mini-batch size .\n","        shuffle: Shuffling the images or not.\n","    Return:\n","        mini_batches_X: a list of all mini-batches images, each element in\n","                        it is an numpy.ndarray containing one batch of images.\n","        mini_batches_Y: a list of all mini-batches one-hot labels, \n","                        each element in it is an one-hot label.\n","    \"\"\"\n","    m = X.shape[0]\n","    mini_batches_X = []\n","    mini_batches_Y = []\n","    \n","    if shuffle:\n","        permutation = list(np.random.permutation(m))\n","        X = X[permutation, ]\n","        Y = Y[permutation, ]\n","        \n","    num_complete_minibathes = int(np.math.floor(m/mini_batch_size))\n","\n","    for k in range(0, num_complete_minibathes):\n","        mini_batch_X = X[k*mini_batch_size:(k+1)*mini_batch_size, ]\n","        mini_batch_Y = Y[k*mini_batch_size:(k+1)*mini_batch_size, ]\n","        \n","        mini_batches_X.append(mini_batch_X)\n","        mini_batches_Y.append(mini_batch_Y)\n","        \n","    if m % mini_batch_size != 0:\n","        mini_batch_X = X[num_complete_minibathes*mini_batch_size:, ]\n","        mini_batch_Y = Y[num_complete_minibathes*mini_batch_size:, ]\n","        mini_batches_X.append(mini_batch_X)\n","        mini_batches_Y.append(mini_batch_Y)\n","\n","    return mini_batches_X, mini_batches_Y"],"metadata":{"id":"WL20PHOokBKN","executionInfo":{"status":"ok","timestamp":1671893545945,"user_tz":-480,"elapsed":2,"user":{"displayName":"salad xx","userId":"13786222985564916523"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## 构建5层CNN模型"],"metadata":{"id":"W8G8_s8Nl9ee"}},{"cell_type":"code","source":["def max_pool(feature_map):\n","    return tf.nn.max_pool(feature_map, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","def conv_relu(feature_map, weight, bias):\n","    conv = tf.nn.conv2d(feature_map, weight, strides=[1, 1, 1, 1], padding='SAME')\n","    return tf.nn.relu(conv + bias)\n","\n","def conv_pool_relu(feature_map, weight, bias):\n","    conv = tf.nn.conv2d(feature_map, weight, strides=[1, 1, 1, 1], padding='SAME')\n","    pool = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","    return tf.nn.relu(pool + bias)\n","\n","def fc_relu(fc_input, weight, bias):\n","    fc = tf.matmul(fc_input, weight) \n","    return tf.nn.relu(fc + bias)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":150},"id":"b1dNM7B3k_Ur","executionInfo":{"status":"error","timestamp":1671893607315,"user_tz":-480,"elapsed":366,"user":{"displayName":"salad xx","userId":"13786222985564916523"}},"outputId":"04a729a2-10d6-46cd-e21a-f906337af26d"},"execution_count":11,"outputs":[{"output_type":"error","ename":"TabError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-546d6f0b687e>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    conv2 = tf.nn.dropout(conv2, keep_prob)\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"]}]},{"cell_type":"code","source":["def cifar10_5layers(input_image, keep_prob, init_method=tf.truncated_normal_initializer(stddev=1e-2)):\n","    \"\"\" \n","    model definition with 5 layers.\n","    Args:\n","        input_image: input image tensor.\n","        init_method: initialization method. \n","                     The default is tf.truncated_normal_initializer(1e-2)\n","    Return:\n","        model: computation graph of the defined model.\n","    \"\"\"\n","    with tf.variable_scope(\"conv1\"):\n","        W1 = tf.get_variable(name=\"W1\", shape=[5,5,3,32], dtype=tf.float32, \n","                             initializer=init_method)\n","        b1 = tf.get_variable(name=\"b1\", shape=[32], dtype=tf.float32, \n","                             initializer=tf.constant_initializer(0.01))\n","        conv1 = conv_pool_relu(input_image, W1, b1)\n","    with tf.variable_scope(\"conv2\"):\n","        W2 = tf.get_variable(name=\"W2\", shape=[5,5,32,64], dtype=tf.float32, \n","                             initializer=init_method)\n","        b2 = tf.get_variable(name=\"b2\", shape=[64], dtype=tf.float32, \n","                             initializer=tf.constant_initializer(0.01))\n","        conv2 = conv_pool_relu(conv1, W2, b2)\n","\t      conv2 = tf.nn.dropout(conv2, keep_prob)\n","    with tf.variable_scope(\"conv3\"):\n","        W3 = tf.get_variable(name=\"W3\", shape=[5,5,64,128], dtype=tf.float32, \n","                             initializer=init_method)\n","        b3 = tf.get_variable(name=\"b3\", shape=[128], dtype=tf.float32, \n","                             initializer=tf.constant_initializer(0.01))\n","        conv3 = conv_pool_relu(conv2, W3, b3)\n","\t      conv3 = tf.nn.dropout(conv3, keep_prob)\n","    with tf.variable_scope(\"fc1\"):\n","        W4 = tf.get_variable(name=\"W4\", shape=[4*4*128,256], dtype=tf.float32, \n","                             initializer=init_method)\n","        b4 = tf.get_variable(name=\"b4\", shape=[256], dtype=tf.float32, \n","                             initializer=tf.constant_initializer(0.01))\n","        conv3_flat = tf.reshape(conv3, [-1, 4*4*128])\n","        fc1 = fc_relu(conv3_flat, W4, b4)\n","\t      fc1 = tf.nn.dropout(fc1, keep_prob)\n","    with tf.variable_scope(\"output\"):\n","        W5 = tf.get_variable(name=\"W5\", shape=[256,10], dtype=tf.float32, \n","                             initializer=init_method)\n","        b5 = tf.get_variable(name=\"b5\", shape=[10], dtype=tf.float32, \n","                             initializer=tf.constant_initializer(0.01))\n","\t      y_logit = tf.matmul(fc1, W5) + b5\n","    return y_logit, tf.nn.softmax(y_logit, name=\"softmax\")"],"metadata":{"id":"OD7gVdzjooGG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 训练模型"],"metadata":{"id":"lFmUBFe_mko2"}},{"cell_type":"code","source":["init_methods = {\"Gaussian\": tf.truncated_normal_initializer(stddev=1e-2), \n","                \"Xavier\": tf.contrib.layers.xavier_initializer_conv2d(),\n","                \"He\": tf.contrib.layers.variance_scaling_initializer()}\n","\n","def parse_args():\n","\t\"\"\"\n","\tparse input arguments.\n","\t\"\"\"\n","\tparse = argparse.ArgumentParser(description=\"CIFAR-10 training\") \n","\tparse.add_argument(\"--model\", dest=\"model_name\", \n","\t\t\t\t\t   help=\"model name: 'cifar10-5layers' or 'cifar10-8layers'\",\n","\t\t\t\t\t   default=\"cifar10-5layers\")\n","\tparse.add_argument(\"--init\", dest=\"init_method\",\n","\t\t\t\t\t   help=\"initialization method for weights, 'Gaussian', 'Xavier' or 'He'\",\n","\t\t\t\t\t   default=\"Gaussian\")\n","\targs = parse.parse_args() # 获取所有的参数\n","\treturn args\n","\n","def main():\n","    \"\"\"\n","    # 下载并解压数据集(已下载可忽略)\n","    data_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n","    data_path = \"./data/\"\n","    download.maybe_download_and_extract(data_url,data_path)\n","    \"\"\"\n","    args = parse_args()\n","    # 导入数据集并显示数据集信息\n","    class_names= load_label_names()\n","    images_train, _, labels_train = load_training_data()\n","    images_test, _, labels_test = load_test_data()\n","    images_train, labels_train, images_valid, labels_valid = \\\n","        split_train_data(images_train, labels_train, shuffle=True)\n","\n","    print(\"classes names:\", class_names)\n","    print(\"shape of training images:\", images_train.shape)\n","    print(\"shape of training labels (one-hot):\", labels_train.shape)\n","    print(\"shape of valid images:\", images_valid.shape)\n","    print(\"shape of valid labels (one-hot):\", labels_valid.shape)\n","    print(\"shape of test images:\", images_test.shape)\n","    print(\"shape of testing labels (one-hot):\", labels_test.shape)\n","\n","    # 将数据集分成mini-batches.\n","    images_train_batches, labels_train_batches = create_mini_batches(images_train, \\\n","                                                                    labels_train, \\\n","                                                                    shuffle=True)\n","    print(\"shape of one batch training images:\", images_train_batches[0].shape)\n","    print(\"shape of one batch training labels:\", labels_train_batches[0].shape)\n","    print(\"shape of last batch training images:\", images_train_batches[-1].shape)\n","    print(\"shape of last batch training labels:\", labels_train_batches[-1].shape)\n","\n","    img_size = images_train.shape[1]\n","    num_channels = images_train.shape[-1]\n","    num_classes = len(class_names)\n","    batch_size = images_train_batches[0].shape[0]\n","    num_batches = len(images_train_batches)\n","\n","    # 创建模型\n","    X = tf.placeholder(tf.float32, [None, img_size, img_size, num_channels])\n","    Y_ = tf.placeholder(tf.float32, [None, num_classes])\n","    keep_prob = tf.placeholder(tf.float32)\n","\n","    init_method = init_methods[args.init_method]\n","    if args.model_name == \"cifar10-5layers\":\n","        logit, Y = cifar10_5layers(X, keep_prob, init_method)\n","    elif args.model_name == \"cifar10-8layers\":\n","        logit, Y = cifar10_8layers(X, keep_prob, init_method)\n","\n","    # 交叉熵损失和准确率\n","    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=Y_)\n","    cross_entropy = tf.reduce_mean(cross_entropy)\n","    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","    # 训练过程\n","    tr_step = []; tr_acc = []; tr_loss = []\n","    va_step = []; va_acc = []; va_loss = []\n","    train_steps =50000 \n","    lr = 0.0001\n","    train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n","    init = tf.global_variables_initializer()\n","    saver = tf.train.Saver()\n","    with tf.Session() as sess:\n","        sess.run(init)\n","        for i in range(train_steps+1):\n","            # 获取一个mini batch数据\n","            j = i%num_batches # j用于记录第几个mini batch\n","            batch_X = images_train_batches[j]\n","            batch_Y = labels_train_batches[j]\n","            if j == num_batches-1: # 遍历一遍训练集（1个epoch）\n","                images_train_batches, labels_train_batches = create_mini_batches(images_train, \\\n","                                                                                labels_train, \\\n","                                                                                shuffle=True)\n","\n","            # 训练一次\n","            sess.run(fetches=train_step, feed_dict={X: batch_X, Y_: batch_Y, keep_prob: 0.5})\n","            \n","            # 每100次打印并记录一组训练结果\n","            if i % 100 == 0:\n","                train_accuracy, train_loss = sess.run([accuracy, cross_entropy], \\\n","                                                      feed_dict={X: batch_X, Y_: batch_Y, keep_prob: 1})\n","                print(\"steps =\", i, \"train loss =\", train_loss, \" train accuracy =\", train_accuracy)\n","                tr_step.append(i)\n","                tr_acc.append(train_accuracy)\n","                tr_loss.append(train_loss)\n","\n","            # 每500次打印并记录一次测试结果（验证集）\n","            if i % 500 == 0:\n","                valid_accuracy, valid_loss = sess.run([accuracy, cross_entropy], \\\n","                                                      feed_dict={X: images_valid, Y_: labels_valid, keep_prob: 1})\n","                va_step.append(i)\n","                va_acc.append(valid_accuracy)\n","                va_loss.append(valid_loss)\n","                print(\"steps =\", i, \"validation loss =\", valid_loss, \" validation accuracy =\", valid_accuracy)\n","            \n","            # 每10000次保存一次训练模型到本地\n","            if i % 10000 == 0 and i > 0:\n","                model_name = args.model_name + \"_\" + args.init_method\n","                model_name = os.path.join(\"./models\", model_name)\n","                saver.save(sess, model_name, global_step=i)\n","\n","    # 保存训练日志到本地  \n","    train_log = \"train_log_\" + args.model_name + \"_\" + args.init_method + \".txt\" \n","    train_log = os.path.join(\"./results\", train_log)\n","    with open(train_log, \"w\") as f:\n","        f.write(\"steps\\t\" + \"accuracy\\t\" + \"loss\\n\")\n","        for i in range(len(tr_step)):\n","            row_data = str(tr_step[i]) + \"\\t\" + str(round(tr_acc[i],3)) + \"\\t\" + str(round(tr_loss[i],3)) + \"\\n\"\n","            f.write(row_data)\n","\n","    valid_log = \"valid_log_\" + args.model_name + \"_\" + args.init_method + \".txt\" \n","    valid_log = os.path.join(\"./results\", valid_log)            \n","    with open(valid_log, \"w\") as f:\n","        f.write(\"steps\\t\" + \"accuracy\\t\" + \"loss\\n\")\n","        for i in range(len(va_step)):\n","            row_data = str(va_step[i]) + \"\\t\" + str(round(va_acc[i],3)) + \"\\t\" + str(round(va_loss[i],3)) + \"\\n\"\n","            f.write(row_data)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"2Pn8zcH1ml-J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 测试模型"],"metadata":{"id":"7HVrVyO9mnZf"}},{"cell_type":"code","source":["def parse_args():\n","    \t\"\"\"\n","\tparse input arguments.\n","\t\"\"\"\n","\tparse = argparse.ArgumentParser(description=\"CIFAR-10 test\") \n","\tparse.add_argument(\"--model\", dest=\"model_name\", \n","\t\t\t\t\t   help=\"model name: 'cifar10-5layers' or 'cifar10-8layers'\",\n","\t\t\t\t\t   default=\"cifar10-5layers\")\n","\tparse.add_argument(\"--path\", dest=\"model_path\",\n","\t\t\t\t\t   help=\"trained model file path: ***.data***\",\n","\t\t\t\t\t   default=\"models/cifar10-5layers_Gaussian-10000.data-00000-of-00001\")\n","\targs = parse.parse_args()\n","\treturn args\n","\n","\n","def main():\n","    args = parse_args()\n","    model_name = args.model_name\n","    model_path = args.model_path.split(\".\")[0]\n","    # 加载测试数据集\n","    images_test, _, labels_test = load_test_data()\n","    print(\"images_test.shape = \", images_test.shape)\n","\n","    # 构建模型（模型也可以直接从.meta文件中恢复）\n","    X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n","    Y_ = tf.placeholder(tf.float32, [None, 10])\n","    keep_prob = tf.placeholder(tf.float32)\n","    if model_name == \"cifar10-5layers\":\n","        _, Y = cifar10_5layers(X, keep_prob)\n","    elif model_name == \"cifar10-8layers\":\n","        _, Y = cifar10_8layers(X, keep_prob)\n","    else:\n","        print(\"wrong model name!\")\n","        print(\"model name: 'cifar10-5layers' or 'cifar10-8layers'\")\n","        raise KeyError\n","\n","    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","    \n","    saver = tf.train.Saver()\n","    with tf.Session() as sess:\n","        # 恢复模型权重\n","        saver.restore(sess, model_path)\n","        # 测试，注意测试阶段keep_prob=1\n","        print(\"test accuracy = \", sess.run(accuracy, feed_dict={X: images_test, Y_: labels_test, keep_prob: 1}))\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"TcdLL4PKmolw"},"execution_count":null,"outputs":[]}]}