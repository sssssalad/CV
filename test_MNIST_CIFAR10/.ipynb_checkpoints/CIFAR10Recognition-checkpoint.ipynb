{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T00:58:21.388918Z",
     "start_time": "2022-12-28T00:58:14.003982Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T00:58:21.435352Z",
     "start_time": "2022-12-28T00:58:21.391481Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 3 #全部数据的迭代次数（整个卷积要轮回的次数）\n",
    "batch_size_train = 64 #每次训练所抓取用于权重更新的数据样的数量\n",
    "batch_size_test = 1000#测试集规模\n",
    "learning_rate = 0.01\n",
    "\n",
    "momentum = 0.5\n",
    "log_interval = 10 #控制可视化输出间隔\n",
    "\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "#是否使用非确定性算法，True时会自动寻找适合当前的高效算法。\n",
    "torch.backends.cudnn.enabled = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchvision.transforms.Compose()类：串联多个图片变换的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchvision.transforms.ToTensor()：将图像PIL.Image或ndarray从 (H x W x C)形状转换为 (C x H x W) 的tensor。\n",
    "\n",
    "[h, w, c]：数组中最外层即hight，表示图像像素有几行；第二层元素width，表示图像像素几列，最后一层元素为每一个通道的数值。\n",
    "\n",
    "[c, h, w]：数组中第一层元素为图像有一个通道，第二层元素为某个通道上的一行像素，第三层为该通道上某列的像素值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms.Normalize(mean, std) 的计算公式：\n",
    "input[channel] = (input[channel] - mean[channel]) / std[channel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enumerate():将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入训练集和测试集，转换为tensor并归一化、分成一个个batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T00:58:21.450811Z",
     "start_time": "2022-12-28T00:58:21.437351Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     torchvision.datasets.MNIST('./data/', train=True, download=True,\n",
    "#                                transform=torchvision.transforms.Compose([\n",
    "#                                    torchvision.transforms.ToTensor(),\n",
    "#                                    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                                ])),\n",
    "#     batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     torchvision.datasets.MNIST('./data/', train=False, download=True,\n",
    "#                                transform=torchvision.transforms.Compose([\n",
    "#                                    torchvision.transforms.ToTensor(),\n",
    "#                                    torchvision.transforms.Normalize(\n",
    "#                                        (0.1307,), (0.3081,))\n",
    "#                                ])),\n",
    "#     batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-28T00:58:14.009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acf23a712e349c5986d790fac2c2187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10('./data/', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ])),\n",
    "    batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10('./data/', train=False, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize(\n",
    "                                       (0.1307,), (0.3081,))\n",
    "                               ])),\n",
    "    batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取一个batch：（样本数量，通道，28,28维）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-28T00:58:14.012Z"
    }
   },
   "outputs": [],
   "source": [
    "examples = enumerate(train_loader)\n",
    "#将训练集train_loader遍历并组合为一个索引序列，同时列出数据和数据标签\n",
    "\n",
    "#next(iterable[, default])： 返回迭代器的下一个项目\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "#print(example_targets)\n",
    "print(example_data.shape)#一个batch有64张黑白像素28*28图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 展示训练集示例及其label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-28T00:58:14.014Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    \"\"\"展示训练集中前6个图像的灰度图像\"\"\"\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Conv2d(in_channels,out_channels,kernel_size,stride = 1,padding = 0,dilation = 1,\n",
    "groups = 1,bias = True,padding_mode = 'zeros')\n",
    "\n",
    "in_channels:输入的四维张量[N, C, H, W]中的C了，即输入张量的channels数\n",
    "\n",
    "out_channels:期望的四维输出张量的channels数\n",
    "\n",
    "kernel_size:卷积核的大小\n",
    "\n",
    "stride = 1：默认为1，卷积核在图像窗口上每次平移的间隔，即所谓的步长\n",
    "\n",
    "Padding：图像填充，后面的int型常数代表填充的多少（行数、列数），默认为0\n",
    "\n",
    "groups = 1：决定了是否采用分组卷积\n",
    "\n",
    "padding_mode = ‘zeros’：即padding的模式，默认采用零填充。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 卷积后得到的图像Dimension=(输入维度−卷积核维度)/步长+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### view（）：相当于reshape（）。\n",
    "其中参数定为-1时代表自动调整这个维度上的元素个数，以保证元素的总数不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-28T00:58:14.016Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化定义模型结构,需要维度相对应\"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        #输入为（batch_size,1,28,28）,设batch_size为n\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        #二维卷积层,卷积核大小5*5，\n",
    "        #1表述输入的数据通道是1,10表示输出的通道是10\n",
    "        #使用10个卷积核就可以得到24 × 24 × 10的卷积层，24=（28-5）/1+1\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        \n",
    "        self.conv2_drop = nn.Dropout2d()#Dropout层，随机扔掉一些神经元\n",
    "        \n",
    "        self.fc1 = nn.Linear(320, 50)#全连接层\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        \"\"\"forward向前推进：定义每一层的输入和输出\n",
    "        F.relu():函数调用\"\"\"\n",
    "    \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))#使用最大池化层，窗口2*2，步长2\n",
    "        #内部conv1输出（n,10,24,24）\n",
    "        #得到池化后的卷积层为12 × 12 × 10，12=(24-2)/2+1，x为（n,10,12,12）。\n",
    "        \n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        #内部conv2输出（64,20，8,8），8=（12-5）/1+1\n",
    "        #得到池化后的卷积层为4×4×20，4=(8-2)/2+1，x为（n,20,4,4）。\n",
    "        \n",
    "        x = x.view(-1, 320)#重新定义矩阵的形状，将数据变成320维的数据即(n,320)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))#通过全连接层1后的输出，x为（n,50）\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.fc2(x)#输出10维，x为（n,10），10个类中属于每个类的概率\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    #在softmax函数的基础上，再进行一次log运算，dim=1保证10个类的概率相加和为1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例化网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-28T00:58:14.018Z"
    }
   },
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)#加入动量的随机梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-28T00:58:14.021Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"定义用于存储数据的容器\"\"\"\n",
    "train_losses = []\n",
    "train_counter = []#训练损失定义的时间点\n",
    "test_losses = []\n",
    "test_counter = [i * len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### network.state_dict()：保存中间的训练状态，保存后以后可从任何一步开始重新训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-28T00:58:14.023Z"
    }
   },
   "outputs": [],
   "source": [
    "#### log_softmax是指在softmax函数的基础上，再进行一次log运算\n",
    "#### nn.NLLLoss():输入是一个对数概率向量和一个目标标签,计算交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-28T00:58:14.024Z"
    }
   },
   "outputs": [],
   "source": [
    "#enumerate(train_loader)将训练集遍历并组合为一个索引序列，列出数据和数据下标\n",
    "#F是torch.nn.functional\n",
    "def train(epoch):\n",
    "    network.train()#对每个epoch调用Net类中继承nn.Module的train方法\n",
    "    \n",
    "    #batch_idx为batch的索引序号\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \"\"\"对每个batch执行的操作\"\"\"\n",
    "        \n",
    "        optimizer.zero_grad()#把优化器的所有梯度0初始化\n",
    "        #若不清0会与上一个batch相关，但实际每个batch并不需要有关联\n",
    "        \n",
    "        output = network(data)#把训练集中的数据导入网络\n",
    "        \n",
    "        loss = F.nll_loss(output, target)#计算梯度：模型输出结果与真实标签的误差\n",
    "        loss.backward()#反向回传\n",
    "        \n",
    "        optimizer.step()#进行单次优化，更新batch中的所有参数\n",
    "        \n",
    "        #结果输出\n",
    "        print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
    "                batch_idx * len(data),len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),loss.item()),\n",
    "                end=\" \" )\n",
    "        train_losses.append(loss.item())\n",
    "        #train_counter记录已训练的数据的个数\n",
    "        train_counter.append((batch_idx * 64) + \n",
    "                             ((epoch - 1) * len(train_loader.dataset)))\n",
    "        \n",
    "        #保存模型中间状态，保存后可从任何一步开始重新训练网络\n",
    "        torch.save(network.state_dict(), './model.pth')#保留中间过程\n",
    "        torch.save(optimizer.state_dict(), './optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-28T00:58:14.027Z"
    }
   },
   "outputs": [],
   "source": [
    "train(1)#指定用一个epoch训练全部数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义测试过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .data.max(1,keepdim=True)[1]中的第一个1表示，按照行来找，找每行的最大值；\n",
    "     [1]表示会返回一个数组，values既c数组中每行最大值，indices是最大值的位置\n",
    "     keepdim=True维持输出的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### view_as（）：返回与给定的tensor大小同形状不同的tensor，只能用于内存中连续存储的张量上。一般用于比较时确保两个比较的数据维度是一致的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-28T00:58:14.030Z"
    }
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()#评价，对每个epoch调用Net类中继承nn.Module的eval方法\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():#测试不需回传参数\n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            output = network(data)\n",
    "            #计算损失总和\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            \n",
    "            #计算准确数\n",
    "            #把target的大小转换为和pred相同的大小\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\r\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-28T00:58:14.032Z"
    }
   },
   "outputs": [],
   "source": [
    "test()  # 不加这个，后面画图就会报错：x and y must be the same size\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-28T00:58:14.034Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 展示测试集示例及其预测类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-28T00:58:14.036Z"
    }
   },
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "#将test_loader遍历并组合为一个索引序列，同时列出数据和数据下标\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "#next(iterable[, default])： 返回迭代器的下一个项目\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = network(example_data)\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Prediction: {}\".format(output.data.max(1, keepdim=True)[1][i].item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从记录点开始训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-28T00:58:14.037Z"
    }
   },
   "outputs": [],
   "source": [
    "continued_network = Net()\n",
    "continued_optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    " \n",
    "network_state_dict = torch.load('model.pth')\n",
    "continued_network.load_state_dict(network_state_dict)\n",
    "optimizer_state_dict = torch.load('optimizer.pth')\n",
    "continued_optimizer.load_state_dict(optimizer_state_dict)\n",
    " \n",
    "# 因为n_epochs=3，上面用了[1, n_epochs + 1)，所以是“4”开始\n",
    "for i in range(4, 9):\n",
    "    test_counter.append(i*len(train_loader.dataset))\n",
    "    train(i)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-28T00:58:14.039Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
